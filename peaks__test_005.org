

* What was checked

- [ ] export from BV
- [ ] peaks correlation
- [ ]


* LINKS

- https://colab.research.google.com/#create=true&language=r
- https://towardsdatascience.com/how-to-use-r-in-google-colab-b6e02d736497
- pandas dataframe mapping
  - https://stackoverflow.com/questions/19226488/change-one-value-based-on-another-value-in-pandas
  - https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict

* PROD: FIRST
** Imports and settings

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

"""
get_ipython().magic("run peaks__test_001.tg.ipy")

"""

get_ipython().magic("load_ext autoreload")
get_ipython().magic("autoreload 2"       )

import os
import sys
import numpy as np
import pandas as pd
import json

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

# get_ipython().magic("matplotlib notebook")
get_ipython().magic("matplotlib inline")

import seaborn as sns

from collections import OrderedDict

import matplotlib as mpt
import mpmath as mmp
import scipy as sp
import pingouin as pg
import outdated as od
import statsmodels as sm
import sklearn as skl
import pandas.util.testing as tm
from scipy import stats as sp
from pingouin import  pairwise_ttests, read_dataset



import deepsensemaking as dsm
from   deepsensemaking.dicts import str_dict,print_dict
from   deepsensemaking.bids  import get_bids_prop
from   deepsensemaking.eeg   import convert_ZZ
from   deepsensemaking.eeg   import convert_ZZ_translators
from   deepsensemaking.eeg   import chan_bund_INEFFICIENT_mean
from   deepsensemaking.pd    import mgrs

from contextlib import ExitStack

from IPython.display import display,HTML,Image

_DIR = dsm.set_cwd()

pd.set_option("display.notebook_repr_html", dsm.base.outside_emacs() )

with ExitStack() as stack:
    [stack.enter_context(mgr) for mgr in mgrs]
    print( pd.get_option("display.max_rows") )


def dispDF(df_TEMP,n=6):
    with ExitStack() as stack:
        [stack.enter_context(mgr) for mgr in mgrs]
        display(df_TEMP.shape)
        display(df_TEMP.sample(n=n).sort_index())





#+END_SRC

** Load setup

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

ifSetup="peaks/data/setup.json"
with open(ifSetup) as fhSetup: setup = OrderedDict(json.load(fhSetup))
print_dict(setup,"setup",1,)


#+END_SRC

** Convert ZZ data

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

ifZ0 = "peaks/data/corr_bv_database_EEG_LONG.csv"
dfZ0 = pd.read_csv(ifZ0)

dfZ4 = convert_ZZ(dfZ0,ifSetup="peaks/data/setup.json",verbose=3,)
dfZ4.shape

ofZ4 = "peaks/data/dfZ4_peaks_crude_CONVERTED.csv"
dfZ4.to_csv(ofZ4,index = False,)


#+END_SRC

** Cummarize peak values (moving from channel to bundle space)

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

# DFs = OrderedDict()
# DFs["C0"] = OrderedDict()
# DFs["C0"]["if0"] = "peaks/data/df0_peaks0_crude.csv"
# DFs["C1"] = OrderedDict()
# DFs["C1"]["if0"] = "peaks/data/df1_peaks1_crude.csv"
# DFs["C2"] = OrderedDict()
# DFs["C2"]["if0"] = "peaks/data/df2_peaks2_crude.csv"
# DFs["C3"] = OrderedDict()
# DFs["C3"]["if0"] = "peaks/data/df3_peaks3_crude.csv"
# DFs["C4"] = OrderedDict()
# DFs["C4"]["if0"] = "peaks/data/df4_peaks4_crude.csv"
# DFs["Z4"] = OrderedDict()
# DFs["Z4"]["if0"] = "peaks/data/dfZ4_peaks_crude_CONVERTED.csv"


DFs = OrderedDict()
DFs["D0"] = OrderedDict()
DFs["D0"]["if0"] = "peaks/data/new_005_df0_peaks0_crude.csv"
DFs["D1"] = OrderedDict()
DFs["D1"]["if0"] = "peaks/data/new_005_df0_peaks0_scipy.csv"
DFs["D2"] = OrderedDict()
DFs["D2"]["if0"] = "peaks/data/new_005_df4_peaks4_crude.csv"
DFs["D3"] = OrderedDict()
DFs["D3"]["if0"] = "peaks/data/new_005_df4_peaks4_scipy.csv"
DFs["Z4"] = OrderedDict()
DFs["Z4"]["if0"] = "peaks/data/dfZ4_peaks_crude_CONVERTED.csv"


for key0 in DFs.keys():
    DFs[key0]["df0"] = pd.read_csv(DFs[key0]["if0"])
    print("got DataFrame for {} ({})".format(key0,DFs[key0]["df0"].shape))

print_dict(DFs,"DFs",1,)


# key0 = "C0"
for key0 in DFs.keys():
    print("processing: {}".format(key0))
    DFs[key0]["df1"] = chan_bund_INEFFICIENT_mean(DFs[key0]["df0"])
    DFs[key0]["of1"] = DFs[key0]["if0"].replace(".csv",".xtra.bund.means.csv")
    DFs[key0]["df1"].to_csv(DFs[key0]["of1"],index=False,)
    print("  DONE and SAVED to: {}".format(DFs[key0]["of1"]))



#+END_SRC

** Save HKL

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

import hickle as hkl
hkl.dump(
    DFs,
    "data/DFs_new_005.hkl.gz",
    mode="w",
    compression="gzip",
)

#+END_SRC

** Inspect columns before unification

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

dispDF(DFs["D3"]["df0"],n=22)

dispDF(DFs["D3"]["df1"],n=22)


#+END_SRC

** Unifiy columns

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

# Keep in Z4 (df0 and df1) only columns that are present in C4
DFs["Z4"]["df0"] = DFs["Z4"]["df0"][ DFs["D4"]["df0"].columns ]
DFs["Z4"]["df1"] = DFs["Z4"]["df1"][ DFs["D4"]["df1"].columns ]


list(DFs["Z4"]["df0"].columns)
list(DFs["Z4"]["df1"].columns)

list(DFs["D4"]["df0"].columns)
list(DFs["D4"]["df1"].columns)


#+END_SRC

** Merge runs for ZZ

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

cols0 = DFs["Z4"]["df0"].columns
cols1 = [col0 for col0 in cols0 if col0 not in ["valX","latX","RUN"]]

print_dict(DFs,"DFs")

DFs["Z0"] = OrderedDict()
DFs["Z0"]["of0"] = "peaks/data/dfZ0_peaks_crude_CONVERTED.csv"

DFs["Z0"]["df0"] = DFs["Z4"]["df0"].groupby(by=cols1,as_index=False).agg("mean")
DFs["Z0"]["df0"]["RUN"] = 0
DFs["Z0"]["df0"] = DFs["Z0"]["df0"][cols0]

DFs["Z0"]["df0"].to_csv(DFs["Z0"]["of0"],index=False,)


dispDF( DFs["Z0"]["df0"], 12 )


DFs["Z0"]["df1"] = chan_bund_INEFFICIENT_mean(DFs["Z0"]["df0"])
DFs["Z0"]["of1"] = DFs["Z0"]["of0"].replace(".csv",".xtra.bund.means.csv")
DFs["Z0"]["df1"].to_csv(DFs["Z0"]["of1"],index=False,)


#+END_SRC

** Rename and drop some columns

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

temp_rename = {
    "CHAN_BUND":"bund0",
    "SUB":"subj0",
    "RUN":"runn0",
}
temp_drop = [
    "SES",
    "TASK",
]

for key0 in DFs.keys():
    print("processing: {}".format(key0))


for key0 in DFs.keys():
    print("processing: {}".format(key0))
    DFs[key0]["df0"].rename(columns=temp_rename,inplace=True,)
    DFs[key0]["df1"].rename(columns=temp_rename,inplace=True,)
    DFs[key0]["df0"].drop(columns=temp_drop,inplace=True,)
    DFs[key0]["df1"].drop(columns=temp_drop,inplace=True,)


for key0 in DFs.keys():
    print("processing: {}".format(key0))
    DFs[key0]["df0"]["set0"] = key0+"chan1"
    DFs[key0]["df1"]["set0"] = key0+"bund1"
    DFs[key0]["df0"].loc[ DFs[key0]["df0"]["bund0"].isnull(),"set0"] = key0+"bund0"


for key0 in DFs.keys():
    print("processing: {}".format(key0))

    assert list(DFs["C0"]["df0"].columns)==list(DFs[key0]["df0"].columns)
    assert list(DFs["C0"]["df0"].columns)==list(DFs[key0]["df1"].columns)


display(list( DFs["C0"]["df0"].columns ))


#+END_SRC

** Checkups

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

for key0 in DFs.keys():
    for sub0 in ["df0","df1",]:
        display("{} {} {}".format(
            key0,
            sub0,
            DFs[key0][sub0].shape,
        ))

for key0 in DFs.keys():
    for sub0 in ["df0","df1",]:
        display("{} {} {}".format(
            key0,
            sub0,
            DFs[key0][sub0].set0.unique(),
        ))

for key0 in DFs.keys():
    for sub0 in ["df0","df1",]:
        display("="*77)
        display("{} {}".format(
            key0,
            sub0,
        ))
        dispDF(  DFs[key0][sub0] )


#+END_SRC

** Merge datasets

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

dfC5 = pd.DataFrame([],columns=DFs["C0"]["df0"].columns)
dfZ5 = pd.DataFrame([],columns=DFs["C0"]["df0"].columns)

for key0 in ["C0","C1","C2","C3","C4",]:
    dfC5 = dfC5.append(DFs[key0]["df0"],ignore_index=True)
    dfC5 = dfC5.append(DFs[key0]["df1"],ignore_index=True)

for key0 in ["Z4","Z0",]:
    dfZ5 = dfZ5.append(DFs[key0]["df0"],ignore_index=True)
    dfZ5 = dfZ5.append(DFs[key0]["df1"],ignore_index=True)

dfC5.set0.unique()
dfZ5.set0.unique()


#+END_SRC

** Filter data to contain only the stuff present in the ZZ data base

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

# dfC5 should keep only `chan0` levels that are present in the corresponding dfZ5 column
# effectively this drops from dfC5 channels that are not of interest
dfC5 = dfC5[ np.isin( dfC5["chan0"], dfZ5["chan0"].unique() ) ]
display(dfC5.shape)
display(dfZ5.shape)
assert sorted(list(dfC5["chan0"].unique())) == sorted(list(dfZ5["chan0"].unique()))


# dfC5 should keep only `cond0` levels that are present in corresponding dfZ5 column
# effectively this drops from dfC5 dummy condition containing all ERPs and
# any conditions based on word length ETC
dfC5 = dfC5[ np.isin( dfC5["cond0"], dfZ5["cond0"].unique() ) ]
display(dfC5.shape)
display(dfZ5.shape)
assert sorted(list(dfC5["cond0"].unique())) == sorted(list(dfZ5["cond0"].unique()))


# BACKUP subjects codes data
# This is used below to explain a missing/misslabeled subject
# This subject (27mwxf/27zgxf) will be removed from this comparison
subjC5 = dfC5["subj0"].unique()
subjZ5 = dfZ5["subj0"].unique()

# Seems that we have an extra subject in the dfZZ database
# Actually the label seems to be mixed up for subject 27mwxf/27zgxf
# I have a vauge memory that we have discussed this isue already
set(subjZ5).difference(set(subjC5))



# dfC5 should keep only `SUB` levels that are present in corresponding dfZ5 column
# effectively this drops from dfC5 subjects not present in dfZ5

# HOT FIX # TODO verify again that this is all hunky-dory
dfZ5["subj0"] = dfZ5["subj0"].str.replace("27mwxf","27zgxf")
dfC5 = dfC5[ np.isin( dfC5["subj0"], dfZ5["subj0"].unique() ) ]
display(dfC5.shape)
display(dfZ5.shape)



assert sorted(dfC5["subj0"].unique())==sorted(dfZ5["subj0"].unique())
assert sorted(dfC5["chan0"].unique())==sorted(dfZ5["chan0"].unique())
assert sorted(dfC5["cond0"].unique())==sorted(dfZ5["cond0"].unique())
assert sorted(dfC5["tmin0"].unique())==sorted(dfZ5["tmin0"].unique())




dispDF(dfC5,22)

dispDF(dfZ5,22)


#+END_SRC

** MERGE ALL

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

dfA0 = dfC5.append(dfZ5, ignore_index=True)
dfA0.shape
dispDF(dfA0,44)

dfA0.to_csv("data/dfA0.csv",index=False)


dfA0.set0.unique()

#+END_SRC

** Intenese checkups

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

# Expected number of basic cases
temp_expect = dict(
    cond0 =  4,
    chan0 = 36,
    tmin0 =  6,
    subj0 = 32,
)
temp_expect = np.prod(list(temp_expect.values()))

# C3chan1 has only one run (0) but two levels for mode0 (pos, neg)
temp_C3chan1_pos1 = len(dfA0.query(""" set0=="C3chan1" & mode0=="pos" """))
assert temp_expect == temp_C3chan1_pos1

# Z4chan1 has only one level for mode0 (pos) but four levels for run (1, 2, 3, 4)
temp_Z3chan1_run1 = len(dfA0.query(""" set0=="Z4chan1" & runn0==1 """ ))
assert temp_expect == temp_Z3chan1_run1

# Analogous to the above
temp_C3bund0_pos1 = len(dfA0.query(""" set0=="C3bund0" & mode0=="pos" """))
temp_C3bund1_pos1 = len(dfA0.query(""" set0=="C3bund1" & mode0=="pos" """))
temp_Z4bund1_run1 = len(dfA0.query(""" set0=="Z4bund1" & runn0==1     """))
temp_Z0chan1_run0 = len(dfA0.query(""" set0=="Z0chan1" & runn0==0     """))
temp_Z0bund1_run0 = len(dfA0.query(""" set0=="Z0bund1" & runn0==0     """))

assert temp_expect == temp_C3bund0_pos1 * 6
assert temp_expect == temp_C3bund0_pos1 * 6
assert temp_expect == temp_Z4bund1_run1 * 6
assert temp_expect == temp_Z0chan1_run0
assert temp_expect == temp_Z0bund1_run0 * 6

display(temp_expect)
display(temp_C3chan1_pos1)
display(temp_Z3chan1_run1)
display(temp_C3bund0_pos1)
display(temp_C3bund1_pos1)
display(temp_Z4bund1_run1)
display(temp_Z0chan1_run0)
display(temp_Z0bund1_run0)

# Each of six bundles contains six channels
display(temp_Z4bund1_run1 * 6)

del temp_expect
del temp_C3chan1_pos1
del temp_Z3chan1_run1
del temp_C3bund0_pos1
del temp_C3bund1_pos1
del temp_Z4bund1_run1
del temp_Z0chan1_run0
del temp_Z0bund1_run0



#+END_SRC

** Rearange columns order

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes

dfA0.columns

dfA0 = dfA0[["set0","mode0","cond0","chan0","bund0","tmin0","runn0","subj0","valX",]]

dispDF(dfA0,24)
display(list(dfA0.columns))
display(len(dfA0.columns))

#+END_SRC


** Introduce redundant factors with redundant levels

#+BEGIN_SRC ipython :session *iPython* :eval yes :results raw drawer :exports both :shebang "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n" :var EMACS_BUFFER_DIR=(file-name-directory buffer-file-name) :tangle yes


ifSetup = "peaks/data/setup.json"
verbose = 0
cond_swaps,tmin_swaps,tmax_swaps,chan_swaps = convert_ZZ_translators(
        ifSetup=ifSetup,verbose=verbose,
    )

print_dict(cond_swaps,"cond_swaps",1,)
print_dict(tmin_swaps,"tmin_swaps",1,)
print_dict(tmax_swaps,"tmax_swaps",1,)
print_dict(chan_swaps,"chan_swaps",1,)


ifSetup="peaks/data/setup.json"
with open(ifSetup) as fhSetup: setup = OrderedDict(json.load(fhSetup))
print_dict(setup,"setup",1,)



chan_infos0 = setup["chans"]["info0"]
print_dict(chan_infos0,"chan_infos0",1,)

chans_later0 = setup["chans"]["info2"]["later0"]
print_dict(chans_later0,"chans_later0",1,)


chans_front0 = setup["chans"]["info2"]["front0"]
print_dict(chans_front0,"chans_front0",1,)


ifA0 = "peaks/data/dfA0.csv"
dfA0 = pd.read_csv(ifA0)
dfA1 = dfA0[["set0","mode0","cond0","chan0","bund0","tmin0","runn0","subj0","valX",]].copy()
display(list(dfA1.columns))


dfA1["later0"] = dfA1["chan0"].map(chans_later0)
dfA1["front0"] = dfA1["chan0"].map(chans_front0)




temp_cols = ["set0","mode0","cond0","chan0","later0","front0","bund0","tmin0","runn0","subj0","valX",]
dfA2 = dfA1[temp_cols]


#+END_SRC
